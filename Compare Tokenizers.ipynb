{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb61d27",
   "metadata": {},
   "source": [
    "Here I am comparing results from different tokenizers available on nltk library. There are about 15 tokenziers I have kept in compare_tokenizers- a user defined function. You can play along and try several other tokenizers available from other libraries ex. Spacy, Gensim, Keras, Textblob etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31bd6a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhitespaceTokenizer: ['Hello', '#World.je', 'suis', 'dans', 'un', 'resto.', 'ä»Šã¾ã§ã§æœ€é«˜ã®ãƒ”ã‚¶ã‚’é£Ÿã¹ã¾ã—ãŸã€‚ğŸ˜...!!']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "WordPunctTokenizer: ['Hello', '#', 'World', '.', 'je', 'suis', 'dans', 'un', 'resto', '.', 'ä»Šã¾ã§ã§æœ€é«˜ã®ãƒ”ã‚¶ã‚’é£Ÿã¹ã¾ã—ãŸ', 'ã€‚ğŸ˜...!!']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "TreebankWordTokenizer: ['Hello', '#', 'World.je', 'suis', 'dans', 'un', 'resto.', 'ä»Šã¾ã§ã§æœ€é«˜ã®ãƒ”ã‚¶ã‚’é£Ÿã¹ã¾ã—ãŸã€‚ğŸ˜', '...', '!', '!']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "PunktSentenceTokenizer: ['Hello #World.je suis dans un resto.', 'ä»Šã¾ã§ã§æœ€é«˜ã®ãƒ”ã‚¶ã‚’é£Ÿã¹ã¾ã—ãŸã€‚ğŸ˜...!', '!']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "RegexpTokenizer: ['Hello', 'World', 'je', 'suis', 'dans', 'un', 'resto', 'ä»Šã¾ã§ã§æœ€é«˜ã®ãƒ”ã‚¶ã‚’é£Ÿã¹ã¾ã—ãŸ']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "SExprTokenizer: ['Hello #World.je suis dans un resto. ä»Šã¾ã§ã§æœ€é«˜ã®ãƒ”ã‚¶ã‚’é£Ÿã¹ã¾ã—ãŸã€‚ğŸ˜...!!']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "LineTokenizer: ['Hello #World.je suis dans un resto. ä»Šã¾ã§ã§æœ€é«˜ã®ãƒ”ã‚¶ã‚’é£Ÿã¹ã¾ã—ãŸã€‚ğŸ˜...!!']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "SpaceTokenizer: ['Hello', '#World.je', 'suis', 'dans', 'un', 'resto.', 'ä»Šã¾ã§ã§æœ€é«˜ã®ãƒ”ã‚¶ã‚’é£Ÿã¹ã¾ã—ãŸã€‚ğŸ˜...!!']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "TweetTokenizer: ['Hello', '#World', '.', 'je', 'suis', 'dans', 'un', 'resto', '.', 'ä»Šã¾ã§ã§æœ€é«˜ã®ãƒ”ã‚¶ã‚’é£Ÿã¹ã¾ã—ãŸ', 'ã€‚', 'ğŸ˜', '...', '!', '!']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "MosesTokenizer: ['Hello', '#', 'World.je', 'suis', 'dans', 'un', 'resto', '.', 'ä»Š', 'ã¾ã§ã§', 'æœ€', 'é«˜', 'ã®ãƒ”ã‚¶ã‚’', 'é£Ÿ', 'ã¹ã¾ã—ãŸ', 'ã€‚', 'ğŸ˜', '...', '!', '!']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "ToktokTokenizer: ['Hello', '#World.je', 'suis', 'dans', 'un', 'resto.', 'ä»Šã¾ã§ã§æœ€é«˜ã®ãƒ”ã‚¶ã‚’é£Ÿã¹ã¾ã—ãŸã€‚ğŸ˜', '...', '!', '!']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "MWETokenizer: ['H', 'e', 'l', 'l', 'o', ' ', '#', 'W', 'o', 'r', 'l', 'd', '.', 'j', 'e', ' ', 's', 'u', 'i', 's', ' ', 'd', 'a', 'n', 's', ' ', 'u', 'n', ' ', 'r', 'e', 's', 't', 'o', '.', ' ', 'ä»Š', 'ã¾', 'ã§', 'ã§', 'æœ€', 'é«˜', 'ã®', 'ãƒ”', 'ã‚¶', 'ã‚’', 'é£Ÿ', 'ã¹', 'ã¾', 'ã—', 'ãŸ', 'ã€‚', 'ğŸ˜', '.', '.', '.', '!', '!']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "SyllableTokenizer: ['Hel', 'lo ', '#', 'World', '.', 'je ', 'sui', 's ', 'dan', 's u', 'n ', 'res', 'to', '.', ' ä»Š', 'ã¾', 'ã§', 'ã§', 'æœ€', 'é«˜', 'ã®', 'ãƒ”', 'ã‚¶', 'ã‚’', 'é£Ÿ', 'ã¹', 'ã¾', 'ã—', 'ãŸ', 'ã€‚', 'ğŸ˜', '.', '', '.', '', '.', '', '!', '!']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "TabTokenizer: ['Hello #World.je suis dans un resto. ä»Šã¾ã§ã§æœ€é«˜ã®ãƒ”ã‚¶ã‚’é£Ÿã¹ã¾ã—ãŸã€‚ğŸ˜...!!']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "BlanklineTokenizer: ['Hello #World.je suis dans un resto. ä»Šã¾ã§ã§æœ€é«˜ã®ãƒ”ã‚¶ã‚’é£Ÿã¹ã¾ã—ãŸã€‚ğŸ˜...!!']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Get multiple outputs in the same cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import (WhitespaceTokenizer,WordPunctTokenizer,TreebankWordTokenizer,\n",
    "                           PunktSentenceTokenizer,RegexpTokenizer,SExprTokenizer,LineTokenizer,\n",
    "                           SpaceTokenizer,TweetTokenizer,ToktokTokenizer,MWETokenizer,SyllableTokenizer,\n",
    "                           TabTokenizer,word_tokenize,sent_tokenize,BlanklineTokenizer)\n",
    "\n",
    "#!pip install -U sacremoses\n",
    "from sacremoses import MosesTokenizer #This one is moved out of the nltk library and can be found in sacremoses.\n",
    "\n",
    "text = \"Hello #World.je suis dans un resto. ä»Šã¾ã§ã§æœ€é«˜ã®ãƒ”ã‚¶ã‚’é£Ÿã¹ã¾ã—ãŸã€‚ğŸ˜...!!\"\n",
    "\n",
    "def compare_tokenizers(text):\n",
    "    tokenizers = [\n",
    "        WhitespaceTokenizer(),\n",
    "        WordPunctTokenizer(),\n",
    "        TreebankWordTokenizer(),\n",
    "        PunktSentenceTokenizer(),\n",
    "        RegexpTokenizer('\\w+'),\n",
    "        SExprTokenizer(),\n",
    "        LineTokenizer(),\n",
    "        SpaceTokenizer(),\n",
    "        TweetTokenizer(),\n",
    "        MosesTokenizer(),\n",
    "        ToktokTokenizer(),\n",
    "        MWETokenizer(),\n",
    "        SyllableTokenizer(),\n",
    "        TabTokenizer(),\n",
    "        BlanklineTokenizer()\n",
    "        ]\n",
    "\n",
    "    for tokenizer in tokenizers:\n",
    "        print(f\"{tokenizer.__class__.__name__}: {tokenizer.tokenize(text)}\")\n",
    "        print(\"-\"*125)\n",
    "        \n",
    "compare_tokenizers(text)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70983a",
   "metadata": {},
   "source": [
    "Depending on your use case you can pick and choose whichever tokenizer best suitable in terms of tokens it generates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
