{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9ecba0",
   "metadata": {},
   "source": [
    "## 1. word_tokenize - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "797cfee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', ',', 'said', 'hello', '!', 'to', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The quick brown fox, said hello! to the lazy dog.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44925cca",
   "metadata": {},
   "source": [
    "- It splits the text into words based on whitespace and punctuation characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64023f3b",
   "metadata": {},
   "source": [
    "## 2. sent_tokenize -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9cf98b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sample sentence.', \"It's meant to be tokenized using a sentence tokenizer.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"This is a sample sentence. It's meant to be tokenized using a sentence tokenizer.\"\n",
    "\n",
    "tokens = sent_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d642869",
   "metadata": {},
   "source": [
    "- Splits text into sentences using a combination of heuristics, such as looking for periods followed by whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd73be0",
   "metadata": {},
   "source": [
    "## 3. WhitespaceTokenizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad7fa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox,', 'said', 'hello!', 'to', 'the', 'lazy', 'dog.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "text = \"The quick brown fox, said hello! to the lazy dog.\"\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43708d7a",
   "metadata": {},
   "source": [
    "- This tokenizer simply splits text into tokens based on whitespace characters such as spaces, tabs, and line breaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df614420",
   "metadata": {},
   "source": [
    "## 4. WordPunctTokenizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "072ef4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'can', \"'\", 't', 'believe', 'it', \"'\", 's', 'not', 'butter', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "text = \"I can't believe it's not butter!\"\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a1456",
   "metadata": {},
   "source": [
    "- This tokenizer splits text into tokens based on whitespace and punctuation characters. \n",
    "- It also splits punctuation characters into separate tokens, so that punctuation marks such as commas and periods are treated as separate tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbf0661",
   "metadata": {},
   "source": [
    "## 5. TreebankWordTokenizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "034dfa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', \"'s\", 'the', 'matter', 'with', 'kids', 'now-a-days', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text = \"What's the matter with kids now-a-days?\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9cd0c1",
   "metadata": {},
   "source": [
    "- This tokenizer is a rule-based tokenizer that follows the conventions used in the Penn Treebank corpus. \n",
    "- It splits off trailing punctuation\n",
    "- Handles contractions and hyphenated words. (e.g. \"can't\", \"won't\", \"mother-in-law\")\n",
    "- Supports multiple languages(e.g. German, French, Spanish)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d12fb81",
   "metadata": {},
   "source": [
    "## 6. RegexpTokenizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071c2148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'Smith', '1.2.1980', 'Los', 'Angeles', 'California', 'He', 'Google', 'Inc', 'New', 'York', 'City']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\b[A-Z][a-z]*\\b|\\d{1,2}\\.\\d{1,2}\\.\\d{2,4}|\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b')\n",
    "text = \"John Smith, PhD, was born on 1.2.1980 in Los Angeles, California. He now works at Google Inc. in New York City.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b09dfc",
   "metadata": {},
   "source": [
    "- By specifying a regular expression pattern that matches the desired tokens, RegexpTokenizer can be customized to suit the specific needs of a particular task or dataset\n",
    "- It allows for more fine-grained control over the tokenization process compared to the built-in word_tokenize function.\n",
    "Here the regular expression matches three types of tokens :\n",
    "    - __A__) <U>Proper nouns</U>: A word starting with an uppercase letter, followed by zero or more lowercase letters (e.g. \"John\", \"Smith\", \"Los\", \"Angeles\", \"California\", \"Google\", \"New\", \"York\", \"City\").\n",
    "    - __B__) <U>Dates in the format \"d.d.yyyy\"</U> : A sequence of two numbers, separated by a period, representing the day and month, followed by a sequence of four numbers representing the year (e.g. \"1.2.1980\").\n",
    "    - __C__) <U>IP addresses</U> : A sequence of four numbers, separated by periods, each number having one to three digits (e.g. \"1.2.198.0\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6c320",
   "metadata": {},
   "source": [
    "## 7. TweetTokenizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "427cc00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'just', 'had', 'the', 'best', 'pizza', 'ever', 'delivered', 'in', 'roguhly', '20-30', 'mins', '!', 'üçï', 'üòç', '#yum', '#pizzalove']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "text = \"I just had the best pizza ever delivered in roguhly 20-30 mins! üçïüòç #yum #pizzalove\"\n",
    "tokenizer = TweetTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682dbd1",
   "metadata": {},
   "source": [
    "- This is designed specifically for processing tweets. It is capable of tokenizing the text of tweets, extracting hashtags, usernames, URLs, and emojis in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5463c8",
   "metadata": {},
   "source": [
    "## 8. ToktokTokenizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9491badf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with word_tokenize:\n",
      " ['Mr.Troy', 'is', 'from', 'the', 'U.S.A', ',', 'and', 'he', 'graduated', 'in', '2010', '.', 'His', 'Email', 'address', 'is', 'troy', '@', 'win_it.com', '.', '#', 'NLP', 'is', 'a', 'fascinating', 'field', '!'] \n",
      "\n",
      "Time taken with word_tokenize:  0.0013442039489746094  seconds\n",
      "*****************************************************************************************************************************\n",
      "Tokens with ToktokTokenizer:\n",
      " ['Mr.Troy', 'is', 'from', 'the', 'U.S.A', ',', 'and', 'he', 'graduated', 'in', '2010.', 'His', 'Email', 'address', 'is', 'troy@win_it.com.', '#NLP', 'is', 'a', 'fascinating', 'field', '!'] \n",
      "\n",
      "Time taken with ToktokTokenizer:  0.00046753883361816406  seconds \n",
      "\n",
      "TokTokTokenizer is  2.88 x is faster\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from nltk.tokenize import word_tokenize, ToktokTokenizer\n",
    "\n",
    "text = \"Mr.Troy is from the U.S.A,and he graduated in 2010. His Email address is troy@win_it.com. #NLP is a fascinating field!\"\n",
    "\n",
    "start_word = time.time()\n",
    "tokens_word_tokenize = word_tokenize(text)\n",
    "end_word = time.time()\n",
    "print(\"Tokens with word_tokenize:\\n\", tokens_word_tokenize,\"\\n\")\n",
    "print(\"Time taken with word_tokenize: \", end_word-start_word, \" seconds\")\n",
    "print(\"*\"*125)\n",
    "start_toktok = time.time()\n",
    "tokenizer = ToktokTokenizer()\n",
    "tokens_toktoktokenizer = tokenizer.tokenize(text)\n",
    "end_toktok = time.time()\n",
    "print(\"Tokens with ToktokTokenizer:\\n\", tokens_toktoktokenizer,\"\\n\")\n",
    "print(\"Time taken with ToktokTokenizer: \", end_toktok-start_toktok, \" seconds \\n\")\n",
    "print(\"TokTokTokenizer is \",round(((end_word-start_word)/(end_toktok-start_toktok)),2),\"x is faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc367738",
   "metadata": {},
   "source": [
    " - The ToktokTokenizer is a fast and efficient tokenizer that is well-suited for tokenizing  large-scale text in languages with whitespace-delimited scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c47a3",
   "metadata": {},
   "source": [
    "## 9. MWETokenizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e60e1afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'New_York', 'and', 'San', 'Francisco.', 'Happy', 'birthday', 'to', 'you!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "# Define a list of multi-word expressions (MWEs)\n",
    "mwe_list = [(\"New\", \"York\"), (\"San\", \"Francisco\"), (\"happy\", \"birthday\")]\n",
    "\n",
    "# Create an instance of the MWETokenizer, passing in the list of MWEs\n",
    "mwe_tokenizer = MWETokenizer(mwe_list)\n",
    "\n",
    "# Tokenize a sentence using the MWETokenizer\n",
    "text = \"I love New York and San Francisco. Happy birthday to you!\"\n",
    "tokens = mwe_tokenizer.tokenize(text.split())\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b3df63",
   "metadata": {},
   "source": [
    "- MWET stands for \"Multi-Word Expression Tokenizer\". It is used for identifing and tokenizing multi-word expressions (MWEs) as a single token, rather than treating them as separate words.\n",
    "- MWEs are expressions made up of multiple words that have a single, specific meaning.(ex. \"Happy Birthday\",\"Hunger Games\")\n",
    "- The MWETokenizer can be useful in a variety of natural language processing tasks, such as named entity recognition, sentiment analysis, and text classification. By preserving the meaning of MWEs in the tokenization process, we can improve the accuracy and effectiveness of these tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
