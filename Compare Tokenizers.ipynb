{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb61d27",
   "metadata": {},
   "source": [
    "Here I am comparing results from different tokenizers available on nltk library. There are about 15 tokenziers I have kept in compare_tokenizers- a user defined function. You can play along and try several other tokenizers available from other libraries ex. Spacy, Gensim, Keras, Textblob etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31bd6a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhitespaceTokenizer: ['I', 'just', 'had', 'the', 'best', 'pizza', 'ever', 'delivered', 'in', 'roguhly', '20-30', 'mins!', 'üçïüòç', '#yum', '#pizzalove']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "WordPunctTokenizer: ['I', 'just', 'had', 'the', 'best', 'pizza', 'ever', 'delivered', 'in', 'roguhly', '20', '-', '30', 'mins', '!', 'üçïüòç', '#', 'yum', '#', 'pizzalove']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "TreebankWordTokenizer: ['I', 'just', 'had', 'the', 'best', 'pizza', 'ever', 'delivered', 'in', 'roguhly', '20-30', 'mins', '!', 'üçïüòç', '#', 'yum', '#', 'pizzalove']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "PunktSentenceTokenizer: ['I just had the best pizza ever delivered in roguhly 20-30 mins!', 'üçïüòç #yum #pizzalove']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "RegexpTokenizer: ['I', 'just', 'had', 'the', 'best', 'pizza', 'ever', 'delivered', 'in', 'roguhly', '20', '30', 'mins', 'yum', 'pizzalove']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "SExprTokenizer: ['I just had the best pizza ever delivered in roguhly 20-30 mins! üçïüòç #yum #pizzalove']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "LineTokenizer: ['I just had the best pizza ever delivered in roguhly 20-30 mins! üçïüòç #yum #pizzalove']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "SpaceTokenizer: ['I', 'just', 'had', 'the', 'best', 'pizza', 'ever', 'delivered', 'in', 'roguhly', '20-30', 'mins!', 'üçïüòç', '#yum', '#pizzalove']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "TweetTokenizer: ['I', 'just', 'had', 'the', 'best', 'pizza', 'ever', 'delivered', 'in', 'roguhly', '20-30', 'mins', '!', 'üçï', 'üòç', '#yum', '#pizzalove']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "MosesTokenizer: ['I', 'just', 'had', 'the', 'best', 'pizza', 'ever', 'delivered', 'in', 'roguhly', '20-30', 'mins', '!', 'üçï', 'üòç', '#', 'yum', '#', 'pizzalove']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "ToktokTokenizer: ['I', 'just', 'had', 'the', 'best', 'pizza', 'ever', 'delivered', 'in', 'roguhly', '20-30', 'mins', '!', 'üçïüòç', '#yum', '#pizzalove']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "MWETokenizer: ['I', ' ', 'j', 'u', 's', 't', ' ', 'h', 'a', 'd', ' ', 't', 'h', 'e', ' ', 'b', 'e', 's', 't', ' ', 'p', 'i', 'z', 'z', 'a', ' ', 'e', 'v', 'e', 'r', ' ', 'd', 'e', 'l', 'i', 'v', 'e', 'r', 'e', 'd', ' ', 'i', 'n', ' ', 'r', 'o', 'g', 'u', 'h', 'l', 'y', ' ', '2', '0', '-', '3', '0', ' ', 'm', 'i', 'n', 's', '!', ' ', 'üçï', 'üòç', ' ', '#', 'y', 'u', 'm', ' ', '#', 'p', 'i', 'z', 'z', 'a', 'l', 'o', 'v', 'e']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "SyllableTokenizer: ['I ', 'jus', 't ', 'ha', 'd t', 'he ', 'bes', 't ', 'piz', 'za ', 'e', 've', 'r ', 'de', 'li', 've', 're', 'd i', 'n ', 'ro', 'gu', 'hly 2', '0', '', '-', '3', '0', ' ', 'mins', '!', ' üçï', 'üòç', ' ', '#', 'yu', 'm ', '#', 'piz', 'za', 'lo', 've']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "TabTokenizer: ['I just had the best pizza ever delivered in roguhly 20-30 mins! üçïüòç #yum #pizzalove']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "BlanklineTokenizer: ['I just had the best pizza ever delivered in roguhly 20-30 mins! üçïüòç #yum #pizzalove']\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Get multiple outputs in the same cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import (WhitespaceTokenizer,WordPunctTokenizer,TreebankWordTokenizer,\n",
    "                           PunktSentenceTokenizer,RegexpTokenizer,SExprTokenizer,LineTokenizer,\n",
    "                           SpaceTokenizer,TweetTokenizer,ToktokTokenizer,MWETokenizer,SyllableTokenizer,\n",
    "                           TabTokenizer,word_tokenize,sent_tokenize,BlanklineTokenizer)\n",
    "\n",
    "#!pip install -U sacremoses\n",
    "from sacremoses import MosesTokenizer #This one is moved out of the nltk library and can be found in sacremoses.\n",
    "\n",
    "text = \"I just had the best pizza ever delivered in roguhly 20-30 mins! üçïüòç #yum #pizzalove\"\n",
    "\n",
    "def compare_tokenizers(text):\n",
    "    tokenizers = [\n",
    "        WhitespaceTokenizer(),\n",
    "        WordPunctTokenizer(),\n",
    "        TreebankWordTokenizer(),\n",
    "        PunktSentenceTokenizer(),\n",
    "        RegexpTokenizer('\\w+'),\n",
    "        SExprTokenizer(),\n",
    "        LineTokenizer(),\n",
    "        SpaceTokenizer(),\n",
    "        TweetTokenizer(),\n",
    "        MosesTokenizer(),\n",
    "        ToktokTokenizer(),\n",
    "        MWETokenizer(),\n",
    "        SyllableTokenizer(),\n",
    "        TabTokenizer(),\n",
    "        BlanklineTokenizer()\n",
    "        ]\n",
    "\n",
    "    for tokenizer in tokenizers:\n",
    "        print(f\"{tokenizer.__class__.__name__}: {tokenizer.tokenize(text)}\")\n",
    "        print(\"-\"*125)\n",
    "        \n",
    "compare_tokenizers(text)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b45a00a",
   "metadata": {},
   "source": [
    "Depending on your use case you can pick and choose whichever tokenizer best suitable in terms of tokens it generates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
